var documenterSearchIndex = {"docs":
[{"location":"functions/#Exported-Functions","page":"Exported Functions","title":"Exported Functions","text":"","category":"section"},{"location":"functions/","page":"Exported Functions","title":"Exported Functions","text":"CurrentModule = FluxArchitectures","category":"page"},{"location":"functions/#Data-Preparation","page":"Exported Functions","title":"Data Preparation","text":"","category":"section"},{"location":"functions/","page":"Exported Functions","title":"Exported Functions","text":"When raw data is available in matrix format, then prepare_data allows for easy conversion and cropping to the format expected by the models. It also accepts inputs compatible to the Tables.jl interface, for example a DataFrame or CSV.File.","category":"page"},{"location":"functions/","page":"Exported Functions","title":"Exported Functions","text":"prepare_data","category":"page"},{"location":"functions/#FluxArchitectures.prepare_data","page":"Exported Functions","title":"FluxArchitectures.prepare_data","text":"prepare_data(data, poollength, datalength, horizon)\nprepare_data(data, poollength, datalength, horizon; normalise=true)\n\nCast 2D time series data into the format used by FluxArchitectures. data is a matrix or  Tables.jl compatible datasource containing data in the form timesteps x features (i.e. each column contains the time series for one feature). poollength defines the number of  timesteps to pool when preparing a single frame of data to be fed to the model. datalength  determines the number of time steps included into the output, and horizon determines the  number of time steps that should be forecasted by the model. The label data is assumed to be  contained in the first column. Outputs features and labels.\n\nNote that when horizon is smaller or equal to poollength, then the model has direct access to the value it is supposed to predict.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Exported Functions","title":"Exported Functions","text":"For loading some example data, the following function can be used.","category":"page"},{"location":"functions/","page":"Exported Functions","title":"Exported Functions","text":"get_data","category":"page"},{"location":"functions/#FluxArchitectures.get_data","page":"Exported Functions","title":"FluxArchitectures.get_data","text":"get_data(dataset, poollength, datalength, horizon)\n\nReturn features and labels from one of the sample datasets in the repository. dataset can be one of :solar, :traffic, :exchange_rate or :electricity. poollength gives  the number of timesteps to pool for the model, datalength determines the number of time steps included into the output, and horizon determines the number of time steps that should be forecasted by the model.\n\nSee also: prepare_data, load_data\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Exported Functions","title":"Exported Functions","text":"The datasets are automatically downloaded when needed. See Datasets for a description.","category":"page"},{"location":"functions/#Models","page":"Exported Functions","title":"Models","text":"","category":"section"},{"location":"functions/","page":"Exported Functions","title":"Exported Functions","text":"The following models are exported:","category":"page"},{"location":"functions/","page":"Exported Functions","title":"Exported Functions","text":"DARNN\nDSANet\nLSTnet\nTPALSTM","category":"page"},{"location":"functions/#FluxArchitectures.DARNN","page":"Exported Functions","title":"FluxArchitectures.DARNN","text":"DARNN(inp, encodersize, decodersize, poollength, orig_idx)\n\nCreate a DA-RNN layer based on the architecture described in Qin et. al., as implemented for PyTorch here. inp specifies the number of input features. encodersize defines the number of LSTM encoder layers, and decodersize defines the number of LSTM decoder layers. poolsize gives the length of the window for the pooled input data, and orig_idx defines the array index where the original time series is stored in the input data,\n\nData is expected as array with dimensions features x poolsize x 1 x data, i.e. for 1000 data points containing 31 features that have been windowed over 6 timesteps, DARNN expects an input size of (31, 6, 1, 1000).\n\nTakes the keyword arguments init and bias for the initialization of the weight vector and bias of the linear layers.\n\n\n\n\n\n","category":"function"},{"location":"functions/#FluxArchitectures.DSANet","page":"Exported Functions","title":"FluxArchitectures.DSANet","text":"DSANet(inp, window, local_length, n_kernels, d_model, d_hid, n_layers, n_head, out=1, drop_prob = 0.1f0, σ = Flux.relu)\n\nCreate a DSANet network based on the architecture described in Siteng Huang et. al.. The code follows the PyTorch implementation. inp specifies the number of input features. window gives the length of the window for the pooled input data. local_length defines the length of the convolution window for the local self attention mechanism. n_kernel defines the number of convolution kernels for both the local and global self attention mechanism. d_hid defines the number of \"hidden\" convolution kernels in the self attention encoder structure. n_layers gives the number of self attention encoders used in the network, and n_head defines the number of attention heads. out gives the number of output time series, drop_prob is the dropout probability for the Dropout layers, and σ defines the network's activation function.\n\nData is expected as array with dimensions features x poolsize x 1 x data, i.e. for 1000 data points containing 31 features that have been windowed over 6 timesteps, DSANet expects an input size of (31, 6, 1, 1000).\n\n\n\n\n\n","category":"function"},{"location":"functions/#FluxArchitectures.LSTnet","page":"Exported Functions","title":"FluxArchitectures.LSTnet","text":"LSTnet(in, convlayersize, recurlayersize, poolsize, skiplength)\nLSTnet(in, convlayersize, recurlayersize, poolsize, skiplength, Flux.relu)\n\nCreate a LSTnet layer based on the architecture described in Lai et. al.. in specifies the number of input features. convlayersize defines the number of convolutional layers, and recurlayersize defines the number of recurrent layers. poolsize gives the length of the window for the pooled input data, and skiplength defines the number of steps the hidden state of the recurrent layer is taken back in time.\n\nData is expected as array with dimensions features x poolsize x 1 x data, i.e. for 1000 data points containing 31 features that have been windowed over 6 timesteps, LSTNet expects an input size of (31, 6, 1, 1000).\n\nTakes the keyword arguments init for the initialization of the recurrent layers; and initW and bias for the initialization of the dense layer.\n\n\n\n\n\n","category":"function"},{"location":"functions/#FluxArchitectures.TPALSTM","page":"Exported Functions","title":"FluxArchitectures.TPALSTM","text":"TPALSTM(in, hiddensize, poollength)\nTPALSTM(in, hiddensize, poollength, layers, filternum, filtersize)\n\nCreate a TPA-LSTM layer based on the architecture described in Shih et. al., as implemented for PyTorch by Jing Wang. in specifies the number of input features. hiddensize defines the input and output size of the LSTM layer, and layers the number of LSTM layers (with standard value 1). filternum and filtersize define the number and size of filters in the attention layer. Standard values are 32 and 1. poolsize gives the length of the window for the pooled input data.\n\nData is expected as array with dimensions features x poolsize x 1 x data, i.e. for 1000 data points containing 31 features that have been windowed over 6 timesteps, TPALSTM expects an input size of (31, 6, 1, 1000).\n\nTakes the keyword arguments initW and bias for the initialization of the Dense layers, and init for the initialization of the StackedLSTM network.\n\n\n\n\n\n","category":"function"},{"location":"models/darnn/#DARNN","page":"DARNN","title":"DARNN","text":"","category":"section"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"The \"Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction\" is based on the paper by Qin et. al.. See also the corresponding blog post.","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"The code is based on a PyTorch implementation of the same model with slight adjustments.","category":"page"},{"location":"models/darnn/#Network-Structure","page":"DARNN","title":"Network Structure","text":"","category":"section"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"The neural network has a rather complex structure. Starting with an encoder-decoder structure, it consists of two units, one called the input attention mechanism, and a temporal attention mechanism.","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"The input attention mechanism feeds the input data to a LSTM network. In subsequent calculations, only its hidden state is used, where additional network layers try to estimate the importance of different hidden variables.\nThe temporal attention mechanism takes the hidden state of the encoder network and combines it with the hidden state of another LSTM decoder. Additional network layers try again to estimate the importance of the hidden variables of the encoder and decoder combined.\nLinear layers combine the output of different layers to the final time series prediction.","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"The encoder part made up of the following:","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"A LSTM encoder layer. Only the hidden state is used for the output of the network.\nA feature attention layer consisting of two Dense layers with tanh activation function for the first layer.","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"(Image: Model Structure Encoder)","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"Encoder Structure. Image from Qin et. al., \"Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction\", ArXiv, 2017.","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"The decoder part consist of","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"A LSTM decoder layer. It's hidden state is used for determining a scaling of the original time series.\nA temporal attention layer operating on the hidden state of the encoder layer, consisting of two Dense layers similar to the encoder.\nA Dense layer operating on the encoder output and the temporal attention layer. Its ouput gets fed into the decoder.\nA Dense layer to obtain the final output based on the hidden state of the decoder.","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"(Image: Decoder Structure Encoder)","category":"page"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"Decoder Structure. Image from Qin et. al., \"Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction\", ArXiv, 2017.","category":"page"},{"location":"models/darnn/#Known-Issues","page":"DARNN","title":"Known Issues","text":"","category":"section"},{"location":"models/darnn/","page":"DARNN","title":"DARNN","text":"Training of the DARNN layer is very slow, probably due to the use of Zygote.Buffer in the encoder.","category":"page"},{"location":"models/lstnet/#LSTnet","page":"LSTnet","title":"LSTnet","text":"","category":"section"},{"location":"models/lstnet/","page":"LSTnet","title":"LSTnet","text":"This \"Long- and Short-term Time-series network\" is based on the paper by Lai et. al.. See also the corresponding blog post.","category":"page"},{"location":"models/lstnet/#Network-Structure","page":"LSTnet","title":"Network Structure","text":"","category":"section"},{"location":"models/lstnet/","page":"LSTnet","title":"LSTnet","text":"(Image: Model Structure)","category":"page"},{"location":"models/lstnet/","page":"LSTnet","title":"LSTnet","text":"Image from Lai et al, \"Long- and Short-term Time-series network\", ArXiv 2017.","category":"page"},{"location":"models/lstnet/","page":"LSTnet","title":"LSTnet","text":"The neural net consists of the following elements:","category":"page"},{"location":"models/lstnet/","page":"LSTnet","title":"LSTnet","text":"A convolutional layer than operates on some window of the time series.\nA GRU cell with relu activation function.\nA SkipGRU similar to the previous GRU cell, with the difference that the hidden state is taken from a specific amount of timesteps back in time. Both the GRU and the SkipGRU layer take their input from the convolutional layer.\nA dense layer that operates on the concatenated output of the previous two layers.\nAn autoregressive layer operating on the input data itself, being added to the output of the dense layer.","category":"page"},{"location":"models/tpalstm/#TPA-LSTM","page":"TPALSTM","title":"TPA-LSTM","text":"","category":"section"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"(Image: Model Structure)","category":"page"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"Image from Shih et. al., \"Temporal Pattern Attention for Multivariate Time Series Forecasting\", ArXiv, 2019.","category":"page"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"The Temporal Pattern Attention LSTM network is based on the paper \"Temporal Pattern Attention for Multivariate Time Series Forecasting\" by Shih et. al.. It claims to have a better performance than LSTnet, with the additional advantage that an attention mechanism automatically tries to determine important parts of the time series, instead of introducing parameters that need to be optimized by the user.","category":"page"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"The code is based on a PyTorch implementation by Jing Wang of the same model with slight adjustments.","category":"page"},{"location":"models/tpalstm/#Network-Structure","page":"TPALSTM","title":"Network Structure","text":"","category":"section"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"The neural net consists of the following elements: The first part consists of an embedding and stacked LSTM layer made up of the following parts:","category":"page"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"A Dense embedding layer for the input data.\nA StackedLSTM layer for the transformed input data.","category":"page"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"The temporal attention mechanism consist of","category":"page"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"A Dense layer that transforms the hidden state of the last LSTM layer in the StackedLSTM.\nA convolutional layer operating on the pooled output of the previous layer, estimating the importance of the different datapoints.\nA Dense layer operating on the LSTM hidden state and the output of the attention mechanism.","category":"page"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"A final Dense layer is used to calculate the output of the network.","category":"page"},{"location":"models/tpalstm/#Stacked-LSTM","page":"TPALSTM","title":"Stacked LSTM","text":"","category":"section"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"The stacked version of a number of LSTM cells is obtained by feeding the hidden state of one cell as input to the next one. Flux.jl's standard setup only allows feeding the output of one cell as the new input, thus we adjust some of the internals:","category":"page"},{"location":"models/tpalstm/","page":"TPALSTM","title":"TPALSTM","text":"Management of hidden states in Flux is done by the Recur structure, which returns the output of a recurrent layer. We use a HiddenRecur structure instead which returns the hidden state.\nThe StackedLSTM-function chains everything together depending on the number of layers. (One layer corresponds to a standard LSTM cell.)","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"CurrentModule = FluxArchitectures","category":"page"},{"location":"examples/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/examples/#LSTnet-Copy-Paste-Code","page":"Examples","title":"LSTnet - Copy-Paste Code","text":"","category":"section"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"If you want to start right away, make sure that FluxArchitectures and Plots are installed, and try the following. Details are below.","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"using FluxArchitectures, Plots\n\n@info \"Loading data\"\npoollength = 10\nhorizon = 15\ndatalength = 1000\ninput, target = get_data(:exchange_rate, poollength, datalength, horizon) |> gpu\n\n@info \"Creating model and loss\"\ninputsize = size(input, 1)\nconvlayersize = 2\nrecurlayersize = 3\nskiplength = 120\nmodel = LSTnet(inputsize, convlayersize, recurlayersize, poollength, skiplength, init=Flux.zeros32, initW=Flux.zeros32) |> gpu\n\nfunction loss(x, y)\n    Flux.ChainRulesCore.ignore_derivatives() do\n        Flux.reset!(model)\n    end\n    return Flux.mse(model(x), y')\nend\n\ncb = function ()\n    Flux.reset!(model)\n    pred = model(input)' |> cpu\n    Flux.reset!(model)\n    p1 = plot(pred, label=\"Predict\")\n    p1 = plot!(cpu(target), label=\"Data\", title=\"Loss $(loss(input, target))\")\n    display(plot(p1))\nend\n\n@info \"Start loss\" loss = loss(input, target)\n@info \"Starting training\"\nFlux.train!(loss, Flux.params(model),Iterators.repeated((input, target), 20), Adam(0.01), cb=cb)\n@info \"Final loss\" loss = loss(input, target)","category":"page"},{"location":"examples/examples/#LSTnet-Step-by-step","page":"Examples","title":"LSTnet - Step-by-step","text":"","category":"section"},{"location":"examples/examples/#Load-some-sample-data","page":"Examples","title":"Load some sample data","text":"","category":"section"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"We start out by loading some of the example Datasets - in this case the :exchange_rate dataset, a collection of daily exchange rates of eight foreign countries. To speed up training, we only take the first 1000 time steps from the data. We would like to feed the model with a window of 10 past timesteps while at the same time trying to forecast 15 timesteps in the future.","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"using FluxArchitectures, Plots\n\npoollength = 10\nhorizon = 15\ndatalength = 1000\ninput, target = get_data(:exchange_rate, poollength, datalength, horizon) |> gpu","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"note: Note\nThe poollength and horizon parameters count \"forward\" in time, which means that when horizon is smaller or equal to poollength, then the model has direct access to the value it is supposed to predict.","category":"page"},{"location":"examples/examples/#Define-the-neural-net-and-loss","page":"Examples","title":"Define the neural net and loss","text":"","category":"section"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"We use a LSTnet model with 2 convolutional layers, and 3 recurrent layers. For the recurrent-skip component, we use a hidden state 120 time steps from the past.","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"inputsize = size(input, 1)\nconvlayersize = 2\nrecurlayersize = 3\nskiplength = 120\nmodel = LSTnet(inputsize, convlayersize, recurlayersize, poollength, skiplength, init=Flux.zeros32, initW=Flux.zeros32) |> gpu","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"As the loss function, we use the standard mean squared error loss. To make sure to reset the hidden state for each training loop, we call Flux.reset! every time we calculate the loss, and wrap it in ignore_derivatives() to exclude the model reset from the derivative calculation.","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"function loss(x, y)\n    Flux.ChainRulesCore.ignore_derivatives() do\n        Flux.reset!(model)\n    end\n    return Flux.mse(model(x), y')\nend","category":"page"},{"location":"examples/examples/#Callback-for-plotting-the-training","page":"Examples","title":"Callback for plotting the training","text":"","category":"section"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"To observe the training progress, we define the following function to plot the training data and prediction together with the current loss value.","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"cb = function ()\n    Flux.reset!(model)\n    pred = model(input)' |> cpu\n    Flux.reset!(model)\n    p1 = plot(pred, label=\"Predict\")\n    p1 = plot!(cpu(target), label=\"Data\", title=\"Loss $(loss(input, target))\")\n    display(plot(p1))\nend","category":"page"},{"location":"examples/examples/#Training-loop","page":"Examples","title":"Training loop","text":"","category":"section"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"Finally, we start the training loop and train for 20 epochs.","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"@info \"Start loss\" loss = loss(input, target)\n@info \"Starting training\"\nFlux.train!(loss, Flux.params(model),Iterators.repeated((input, target), 20),\n            Adam(0.01), cb=cb)\n@info \"Final loss\" loss = loss(input, target)","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"(Image: LSTnetTrainingExample)","category":"page"},{"location":"examples/examples/#DARNN-Example","page":"Examples","title":"DARNN Example","text":"","category":"section"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"Use the following settings as as starting point:","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"poollength = 10\nhorizon = 15\ndatalength = 500\ninput, target = get_data(:solar, poollength, datalength, horizon) |> gpu\n\ninputsize = size(input, 1)\nencodersize = 10\ndecodersize = 10\n\nmodel = DARNN(inputsize, encodersize, decodersize, poollength, 1) |> gpu","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"and train with Adam(0.007) as optimizer.","category":"page"},{"location":"examples/examples/#DSANet-Example","page":"Examples","title":"DSANet Example","text":"","category":"section"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"DSANet suffers from some numerical instabilities - it can be advisable to try initializing the model with different random seeds. The following settings give an example.","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"poollength = 10\nhorizon = 15\ndatalength = 4000\ninput, target = get_data(:exchange_rate, poollength, datalength, horizon) |> gpu\n\ninputsize = size(input, 1)\nlocal_length = 3\nn_kernels = 3\nd_model = 4\nhiddensize = 1\nn_layers = 3\nn_head = 2\n\nRandom.seed!(123)\nmodel = DSANet(inputsize, poollength, local_length, n_kernels, d_model, hiddensize, n_layers, n_head) |> gpu","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"Use Adam(0.005) as optimizer.","category":"page"},{"location":"examples/examples/#TPALSTM-Example","page":"Examples","title":"TPALSTM Example","text":"","category":"section"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"Use the following settings on the example data:","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"poollength = 10\nhorizon = 15\ndatalength = 2000\ninput, target = get_data(:solar, poollength, datalength, horizon) |> gpu\n\ninputsize = size(input, 1)\nhiddensize = 10\nlayers = 2\nfilternum = 32\nfiltersize = 1\n\nmodel = TPALSTM(inputsize, hiddensize, poollength, layers, filternum, filtersize) |> gpu","category":"page"},{"location":"examples/examples/","page":"Examples","title":"Examples","text":"Train with Adam(0.02).","category":"page"},{"location":"models/dsanet/#DSANet","page":"DSANet","title":"DSANet","text":"","category":"section"},{"location":"models/dsanet/","page":"DSANet","title":"DSANet","text":"This \"Dual Self-Attention Network for Multivariate Time Series Forecasting\" is based on the paper by Siteng Huang et. al..","category":"page"},{"location":"models/dsanet/","page":"DSANet","title":"DSANet","text":"The code is based on a PyTorch implementation of the same model with slight adjustments.","category":"page"},{"location":"models/dsanet/#Network-Structure","page":"DSANet","title":"Network Structure","text":"","category":"section"},{"location":"models/dsanet/","page":"DSANet","title":"DSANet","text":"(Image: Network Structure)","category":"page"},{"location":"models/dsanet/","page":"DSANet","title":"DSANet","text":"The neural net consists of the following elements:","category":"page"},{"location":"models/dsanet/","page":"DSANet","title":"DSANet","text":"An autoregressive part\nA local temporal convolution mechanism, fed to a self-attention structure.\nA global temporal convolution mechanism, fed to a self-attention structure.","category":"page"},{"location":"models/dsanet/#Known-Issues","page":"DSANet","title":"Known Issues","text":"","category":"section"},{"location":"models/dsanet/","page":"DSANet","title":"DSANet","text":"The MaxPool pooling layers in the temporal convolution mechanisms can cause the model output to become NaN during training. This is not captured yet. Changing the model parameters or the random seed before training can help.\nThe original model transposes some internal matrices, using Flux.batched_transpose. As there is no adjoint defined yet for this operation in Zygote.jl, we use permutedims instead.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = FluxArchitectures","category":"page"},{"location":"reference/#FluxArchitectures","page":"Reference","title":"FluxArchitectures","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [FluxArchitectures]","category":"page"},{"location":"reference/#FluxArchitectures.HiddenRecur","page":"Reference","title":"FluxArchitectures.HiddenRecur","text":"HiddenRecur(cell)\n\nHiddenRecur takes a recurrent cell and makes it stateful, managing the hidden state in the background. As opposed to Recur, it returns the both the hidden state and the cell state.\n\nSee also: Flux.Recur\n\n\n\n\n\n","category":"type"},{"location":"reference/#FluxArchitectures.Reg_LayerNorm","page":"Reference","title":"FluxArchitectures.Reg_LayerNorm","text":"Reg_LayerNorm(h::Integer)\n\nA normalisation layer designed to be used with recurrent hidden states of size h. Normalises the mean and standard deviation of each input before applying a per-neuron gain/bias. To avoid numeric overflow, the division by the standard deviation has been regularised by adding ϵ = 1E-5.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FluxArchitectures.Seq","page":"Reference","title":"FluxArchitectures.Seq","text":"Seq(RNN)\n\nSeq takes a recurrent neural network and \"sequentializes\" it, i.e. when Seq(RNN) is called with a matrix of input features over a certain time interval, the recurrent neural net is fed with a sequence of inputs, and results are transformed back to matrix form.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FluxArchitectures.SeqSkip","page":"Reference","title":"FluxArchitectures.SeqSkip","text":"SeqSkip(RNNCell, skiplength::Integer)\n\nSeqSkip takes a recurrent neural network cell and \"sequentializes\" it, i.e. when it is called with a matrix of input features over a certain time interval, the recurrent neural net is fed with a sequence of inputs, and results are transformed back to matrix form. In addition, the hidden state from skiplength timesteps ago is used instead of the current hidden state. This structure combines functionality of Recur in that it makes a recurrent neural network cell stateful, as well as Seq in that it feeds matrices of input features as elements of a time series.\n\nSee also: Seq, Flux.Recur\n\n\n\n\n\n","category":"type"},{"location":"reference/#FluxArchitectures.DARNN-NTuple{5, Integer}","page":"Reference","title":"FluxArchitectures.DARNN","text":"DARNN(inp, encodersize, decodersize, poollength, orig_idx)\n\nCreate a DA-RNN layer based on the architecture described in Qin et. al., as implemented for PyTorch here. inp specifies the number of input features. encodersize defines the number of LSTM encoder layers, and decodersize defines the number of LSTM decoder layers. poolsize gives the length of the window for the pooled input data, and orig_idx defines the array index where the original time series is stored in the input data,\n\nData is expected as array with dimensions features x poolsize x 1 x data, i.e. for 1000 data points containing 31 features that have been windowed over 6 timesteps, DARNN expects an input size of (31, 6, 1, 1000).\n\nTakes the keyword arguments init and bias for the initialization of the weight vector and bias of the linear layers.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FluxArchitectures.DSANet","page":"Reference","title":"FluxArchitectures.DSANet","text":"DSANet(inp, window, local_length, n_kernels, d_model, d_hid, n_layers, n_head, out=1, drop_prob = 0.1f0, σ = Flux.relu)\n\nCreate a DSANet network based on the architecture described in Siteng Huang et. al.. The code follows the PyTorch implementation. inp specifies the number of input features. window gives the length of the window for the pooled input data. local_length defines the length of the convolution window for the local self attention mechanism. n_kernel defines the number of convolution kernels for both the local and global self attention mechanism. d_hid defines the number of \"hidden\" convolution kernels in the self attention encoder structure. n_layers gives the number of self attention encoders used in the network, and n_head defines the number of attention heads. out gives the number of output time series, drop_prob is the dropout probability for the Dropout layers, and σ defines the network's activation function.\n\nData is expected as array with dimensions features x poolsize x 1 x data, i.e. for 1000 data points containing 31 features that have been windowed over 6 timesteps, DSANet expects an input size of (31, 6, 1, 1000).\n\n\n\n\n\n","category":"function"},{"location":"reference/#FluxArchitectures.Global_SelfAttn","page":"Reference","title":"FluxArchitectures.Global_SelfAttn","text":"Global_SelfAttn(inp, window, n_kernels, w_kernel, d_model, d_hid, n_layers, n_head)\n\nGlobal self attention module for DSANet. For parameters see DSANet.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FluxArchitectures.LSTnet","page":"Reference","title":"FluxArchitectures.LSTnet","text":"LSTnet(in, convlayersize, recurlayersize, poolsize, skiplength)\nLSTnet(in, convlayersize, recurlayersize, poolsize, skiplength, Flux.relu)\n\nCreate a LSTnet layer based on the architecture described in Lai et. al.. in specifies the number of input features. convlayersize defines the number of convolutional layers, and recurlayersize defines the number of recurrent layers. poolsize gives the length of the window for the pooled input data, and skiplength defines the number of steps the hidden state of the recurrent layer is taken back in time.\n\nData is expected as array with dimensions features x poolsize x 1 x data, i.e. for 1000 data points containing 31 features that have been windowed over 6 timesteps, LSTNet expects an input size of (31, 6, 1, 1000).\n\nTakes the keyword arguments init for the initialization of the recurrent layers; and initW and bias for the initialization of the dense layer.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FluxArchitectures.Local_SelfAttn","page":"Reference","title":"FluxArchitectures.Local_SelfAttn","text":"Local_SelfAttn(inp, window, local_length, n_kernels, w_kernel, d_model, d_hid, n_layers, n_head)\n\nLocal self attention module for DSANet. For parameters see DSANet.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FluxArchitectures.ReluGRU-Tuple","page":"Reference","title":"FluxArchitectures.ReluGRU","text":"ReluGRU(in::Integer, out::Integer)\n\nGated Recurrent Unit layer with relu as activation function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FluxArchitectures.Scaled_Dot_Product_Attention","page":"Reference","title":"FluxArchitectures.Scaled_Dot_Product_Attention","text":"Scaled_Dot_Product_Attention(q, k, v, temperature)\n\nScaled dot product attention function with query q, keys k and values v. Normalisation is given by temperature. Outputs mathrmsoftmaxleft( fracq cdot k^Tmathrmtemperature right)cdot v.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FluxArchitectures.SelfAttn_Encoder","page":"Reference","title":"FluxArchitectures.SelfAttn_Encoder","text":"SelfAttn_Encoder(inp, d_model, n_head, d_hid, drop_prob = 0.1f0, σ = Flux.relu)\n\nEncoder part for the self attention networks that comprise the DSANet. For parameters see DSANet.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FluxArchitectures.SkipGRU-Tuple{Any, Any, Any}","page":"Reference","title":"FluxArchitectures.SkipGRU","text":"SkipGRU(in::Integer, out::Integer, p::Integer)\n\nSkip Gated Recurrent Unit layer with skip length p. The hidden state is recalled from p steps prior to the current calculation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FluxArchitectures.StackedLSTM-NTuple{4, Integer}","page":"Reference","title":"FluxArchitectures.StackedLSTM","text":"StackedLSTM(in, out, hiddensize, layers)\n\nStacked LSTM network. Feeds the data through a chain of LSTM layers, where the hidden state of the previous layer gets fed to the next one. The first layer corresponds to LSTM(in, hiddensize), the hidden layers to LSTM(hiddensize, hiddensize), and the final layer to LSTM(hiddensize, out). Takes the keyword argument init for the initialization of the layers.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FluxArchitectures.TPALSTM","page":"Reference","title":"FluxArchitectures.TPALSTM","text":"TPALSTM(in, hiddensize, poollength)\nTPALSTM(in, hiddensize, poollength, layers, filternum, filtersize)\n\nCreate a TPA-LSTM layer based on the architecture described in Shih et. al., as implemented for PyTorch by Jing Wang. in specifies the number of input features. hiddensize defines the input and output size of the LSTM layer, and layers the number of LSTM layers (with standard value 1). filternum and filtersize define the number and size of filters in the attention layer. Standard values are 32 and 1. poolsize gives the length of the window for the pooled input data.\n\nData is expected as array with dimensions features x poolsize x 1 x data, i.e. for 1000 data points containing 31 features that have been windowed over 6 timesteps, TPALSTM expects an input size of (31, 6, 1, 1000).\n\nTakes the keyword arguments initW and bias for the initialization of the Dense layers, and init for the initialization of the StackedLSTM network.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FluxArchitectures.get_data-NTuple{4, Any}","page":"Reference","title":"FluxArchitectures.get_data","text":"get_data(dataset, poollength, datalength, horizon)\n\nReturn features and labels from one of the sample datasets in the repository. dataset can be one of :solar, :traffic, :exchange_rate or :electricity. poollength gives  the number of timesteps to pool for the model, datalength determines the number of time steps included into the output, and horizon determines the number of time steps that should be forecasted by the model.\n\nSee also: prepare_data, load_data\n\n\n\n\n\n","category":"method"},{"location":"reference/#FluxArchitectures.load_data-Tuple{Any}","page":"Reference","title":"FluxArchitectures.load_data","text":"load_data(dataset)\n\nLoad the raw data from one of the available datasets. The following example data from https://github.com/laiguokun/multivariate-time-series-data  is included:\n\n:solar: The raw data is coming from http://www.nrel.gov/grid/solar-power-data.html:  It contains the solar power production records in the year of 2006, which is sampled every  10 minutes from 137 PV plants in Alabama State.\n:traffic: The raw data is coming from http://pems.dot.ca.gov. The data in this repo is a  collection of 48 months (2015-2016) hourly data from the California Department of  Transportation. The data describes the road occupancy rates (between 0 and 1) measured by  different sensors on San Francisco Bay area freeways.\n:electricity: The raw dataset is from https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014.  It is the electricity consumption in kWh was recorded every 15 minutes from 2011 to 2014  for 321 clients. The data has been cleaned and converted to hourly consumption.\n:exchange_rate: The collection of daily exchange rates of eight foreign countries  including Australia, Great Britain, Canada, Switzerland, China, Japan, New Zealand and  Singapore ranging from 1990 to 2016.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FluxArchitectures.prepare_data-Tuple{AbstractMatrix, Any, Any, Any}","page":"Reference","title":"FluxArchitectures.prepare_data","text":"prepare_data(data, poollength, datalength, horizon)\nprepare_data(data, poollength, datalength, horizon; normalise=true)\n\nCast 2D time series data into the format used by FluxArchitectures. data is a matrix or  Tables.jl compatible datasource containing data in the form timesteps x features (i.e. each column contains the time series for one feature). poollength defines the number of  timesteps to pool when preparing a single frame of data to be fed to the model. datalength  determines the number of time steps included into the output, and horizon determines the  number of time steps that should be forecasted by the model. The label data is assumed to be  contained in the first column. Outputs features and labels.\n\nNote that when horizon is smaller or equal to poollength, then the model has direct access to the value it is supposed to predict.\n\n\n\n\n\n","category":"method"},{"location":"benchmark/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"FluxArchitectures is included in the FluxBench-suite, with results available at speed.fluxml.ai. ","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"To run the benchmarks yourself, add FluxBench as a package and type","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"using FluxBench\nFluxBench.submit()","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"The following sections give the timings for my own hardware (which is very limited as compared to current state-of-the-art equipment).","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"note: Note\nThe setup does not ensure consistency in the number of parameters etc. between models - comparisons are only useful between GPU and CPU runs of the same model, and runs for different versions of FluxArchitectures. In addition, as the benchmark only performs a single backward pass, differences between CPU and GPU are not necessarily indicative of actual training performance. ","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"Currently, only DSANet sees an improvement in training speed on a GPU, whereas all other models train faster on a CPU. The deeper reason is the current limitation of Flux's implementation of RNNs, see this issue that affects all models except DSANet, which doesn't use recurrence. ","category":"page"},{"location":"benchmark/#JetsonNano","page":"Benchmarks","title":"JetsonNano","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"The benchmark is run on a JetsonNano Developer Kit with 4GB of memory.","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":" CPU - Forward GPU - Forward CPU - Backward GPU - Backward\nDARNN 91.290 ms 13.808 s 1.810 s 39.654 s\nDSAnet 222.675ms 556.588 ms 796.776 ms 1.409 s\nLSTNet 18.256 ms 1.452 s 428.595 ms 7.799 s\nTPA-LSTM 23.069 ms 5.632 s 1.572 s 18.750 s","category":"page"},{"location":"benchmark/#Intel-Core-i5-(2.9-GHz)","page":"Benchmarks","title":"Intel Core i5 (2.9 GHz)","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"The benchmark is run on a 13\" MacBook Pro (Late 2016) without GPU support.","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":" CPU - Forward CPU - Backward\nDARNN 29.191 ms 528.621 ms\nDSAnet 137.335 ms 395.298 ms\nLSTNet 3.455 ms 103.729 ms\nTPA-LSTM 7.885 ms 435.726 ms","category":"page"},{"location":"benchmark/#JuliaHub","page":"Benchmarks","title":"JuliaHub","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"GPU enabled cloud computing via JuliaHub.","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":" CPU - Forward GPU - Forward CPU - Backward GPU - Backward\n CPU GPU CPU GPU\nDARNN 57.445 ms 3.939 s 1.005 s 11.189 s\nDSAnet 120.017 ms 57.983 s 541.940 ms 155.569 ms\nLSTNet 3.970 ms 384.773 ms 131.375 s 2.192 s\nTPA-LSTM 7.812 ms 1.435 s 518.579 ms 5.170 s","category":"page"},{"location":"benchmark/#Technical-Details","page":"Benchmarks","title":"Technical Details","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"Version information for Julia and CUDA for the different devices for reference.","category":"page"},{"location":"benchmark/#JetsonNano-2","page":"Benchmarks","title":"JetsonNano","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"julia> versioninfo(verbose=true)\nJulia Version 1.6.2\nCommit 1b93d53fc4 (2021-07-14 15:36 UTC)\nPlatform Info:\n  OS: Linux (aarch64-unknown-linux-gnu)\n      Ubuntu 18.04.6 LTS\n  uname: Linux 4.9.140-tegra #1 SMP PREEMPT Tue Oct 27 21:02:37 PDT 2020 aarch64 aarch64\n  CPU: unknown: \n              speed         user         nice          sys         idle          irq\n       #1   710 MHz      12735 s          0 s        562 s      12506 s        150 s\n       #2   518 MHz       2904 s          0 s        337 s      22803 s         31 s\n       #3  1479 MHz       2901 s          0 s        382 s      22706 s         35 s\n       #4  1479 MHz       2701 s          0 s        350 s      22992 s         22 s\n       \n  Memory: 3.8712120056152344 GB (178.02734375 MB free)\n  Uptime: 2629.0 sec\n  Load Avg:  0.48  0.95  0.97\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-11.0.1 (ORCJIT, cortex-a57)\nEnvironment:\n  HOME = /home/sdobber\n  TERM = xterm-256color\n  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/sdobber/Programme/julia-1.6.2/bin","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"julia> FluxBench.Flux.CUDA.versioninfo()\nCUDA toolkit 10.2.89, local installation\nCUDA driver 10.2.0\n\nLibraries: \n- CUBLAS: 10.2.2\n- CURAND: 10.1.2\n- CUFFT: 10.1.2\n- CUSOLVER: 10.3.0\n- CUSPARSE: 10.3.1\n- CUPTI: 12.0.0\n- NVML: missing\n- CUDNN: 8.0.0 (for CUDA 10.2.0)\n- CUTENSOR: missing\n\nToolchain:\n- Julia: 1.6.2\n- LLVM: 11.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5\n- Device capability support: sm_30, sm_32, sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75\n\n1 device:\n  0: NVIDIA Tegra X1 (sm_53, 66.707 MiB / 3.871 GiB available)","category":"page"},{"location":"benchmark/#Intel-Core-i5-(2.9-GHz-)","page":"Benchmarks","title":"Intel Core i5 (2.9 GHz )","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"julia> versioninfo(verbose=true)\nJulia Version 1.6.2\nCommit 1b93d53fc4 (2021-07-14 15:36 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin18.7.0)\n  uname: Darwin 18.7.0 Darwin Kernel Version 18.7.0: Tue Jun 22 19:37:08 PDT 2021; root:xnu-4903.278.70~1/RELEASE_X86_64 x86_64 i386\n  CPU: Intel(R) Core(TM) i5-6267U CPU @ 2.90GHz: \n              speed         user         nice          sys         idle          irq\n       #1  2900 MHz     206462 s          0 s      94219 s     732080 s          0 s\n       #2  2900 MHz      70845 s          0 s      29756 s     931955 s          0 s\n       #3  2900 MHz     210310 s          0 s      72773 s     749481 s          0 s\n       #4  2900 MHz      60839 s          0 s      25704 s     946013 s          0 s\n       \n  Memory: 16.0 GB (5212.79296875 MB free)\n  Uptime: 1.118083e6 sec\n  Load Avg:  1.67041015625  1.7412109375  1.875\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-11.0.1 (ORCJIT, skylake)\nEnvironment:\n  JULIA_EDITOR = code\n  JULIA_NUM_THREADS = 4\n  HOME = /Users/sdobber\n  PATH = /usr/local/lib/ruby/gems/2.6.0/bin:/Users/sdobber/.gem/ruby/X.X.0/bin:/usr/local/opt/ruby/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Library/TeX/texbin:/opt/X11/bin\n  XPC_FLAGS = 0x0\n  TERM = xterm-256color","category":"page"},{"location":"benchmark/#JuliaHub-2","page":"Benchmarks","title":"JuliaHub","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"julia> versioninfo(verbose=true)\nJulia Version 1.6.1\nCommit 6aaedecc44 (2021-04-23 05:59 UTC)\nPlatform Info:\n  OS: Linux (x86_64-pc-linux-gnu)\n  uname: Linux 4.14.214-160.339.amzn2.x86_64 #1 SMP Sun Jan 10 05:53:05 UTC 2021 x86_64 x86_64\n  CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz: \n              speed         user         nice          sys         idle          irq\n       #1  2701 MHz       7954 s        769 s       3085 s     521963 s          0 s\n       #2  2700 MHz       8465 s        911 s       3105 s     521471 s          0 s\n       #3  2699 MHz       8316 s        735 s       3098 s     521671 s          0 s\n       #4  2697 MHz       7440 s        632 s       3064 s     522231 s          0 s\n       \n  Memory: 59.95986557006836 GB (51687.2734375 MB free)\n  Uptime: 53773.0 sec\n  Load Avg:  1.0  3.33  2.42\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-11.0.1 (ORCJIT, broadwell)\nEnvironment:\n  JULIAHUB_USEREMAIL = \n  JULIAHUB_HOME = /opt/juliahub\n  JULIA_WORKER_TIMEOUT = 360.0\n  JULIARUN_DATA_FOLDER = 378548374194874040\n  JULIARUN_JOB_ID = ooruudwbox\n  JULIA_GR_PROVIDER = BinaryBuilder\n  JULIAHUB_NAMESPACE = 378548374194874040\n  JULIA_NEW_PKG_SERVER = https://juliahub.com/\n  JULIA_DATASETS_PATH = /var/run/secrets/jr-ooruudwboxsecret/DATA_TOML:/opt/juliahub/JuliaHubDataDriver.toml:@:\n  JULIAHUB_USERNAME = Sören Dobberschütz\n  JULIA_DEPOT_PATH = /home/jrun/data/.julia:/home/jrun/.julia\n  JULIARUN_RUN_MODE = script\n  JULIA_HOME = /home/jrun/data/.julia\n  JULIATEAM_HOSTNAME = juliahub.com\n  JULIARUN_RESTART_POLICY = Never\n  JULIA_PKG_SERVER = juliahub.com\n  JULIA_NUM_THREADS = 4\n  JULIA_EDITOR = \"/opt/codeserver/lib/code-server/lib/node\"\n  JULIAHUB_HOME = /opt/juliahub\n  HOME = /home/jrun\n  JULIA_DATASETS_PATH = /var/run/secrets/jr-ooruudwboxsecret/DATA_TOML:/opt/juliahub/JuliaHubDataDriver.toml:@:\n  TERM = xterm-256color\n  JULIA_DEPOT_PATH = /home/jrun/data/.julia:/home/jrun/.julia\n  LD_LIBRARY_PATH = /usr/lib/x86_64-linux-gnu:/opt/codeserver/lib\n  JULIA_HOME = /home/jrun/data/.julia\n  JRUN_APP_BASE_PATH = /\n  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/opt/julia/bin:/usr/lib/x86_64-linux-gnu","category":"page"},{"location":"benchmark/","page":"Benchmarks","title":"Benchmarks","text":"julia> Flux.CUDA.versioninfo()\n┌ Warning: The NVIDIA driver on this system only supports up to CUDA 11.0.0.\n│ For performance reasons, it is recommended to upgrade to a driver that supports CUDA 11.2 or higher.\n└ @ CUDA ~/data/.julia/packages/CUDA/DL5Zo/src/initialization.jl:42\nCUDA toolkit 11.3.1, artifact installation\nCUDA driver 11.0.0\nNVIDIA driver 450.51.6\n\nLibraries: \n- CUBLAS: 11.5.1\n- CURAND: 10.2.4\n- CUFFT: 10.4.2\n- CUSOLVER: 11.1.2\n- CUSPARSE: 11.6.0\n- CUPTI: 14.0.0\n- NVML: 11.0.0+450.51.6\n- CUDNN: 8.20.2 (for CUDA 11.4.0)\n  Downloaded artifact: CUTENSOR\n- CUTENSOR: 1.3.0 (for CUDA 11.2.0)\n\nToolchain:\n- Julia: 1.6.1\n- LLVM: 11.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n\n1 device:\n  0: Tesla K80 (sm_37, 11.170 GiB / 11.173 GiB available)","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = FluxArchitectures","category":"page"},{"location":"#FluxArchitectures","page":"Home","title":"FluxArchitectures","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for FluxArchitectures.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Download Julia 1.6 or later, if you haven't already. You can add FluxArchitectures from  Julia's package manager, by typing ","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add FluxArchitectures","category":"page"},{"location":"","page":"Home","title":"Home","text":"in the Julia prompt.","category":"page"},{"location":"#Models","page":"Home","title":"Models","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LSTnet: This \"Long- and Short-term Time-series network\" follows the paper by Lai et. al..\nDARNN: The \"Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction\" is based on the paper by Qin et. al..\nTPA-LSTM: The Temporal Pattern Attention LSTM network is based on the paper \"Temporal Pattern Attention for Multivariate Time Series Forecasting\" by Shih et. al..\nDSANet: The \"Dual Self-Attention Network for Multivariate Time Series Forecasting\" is based on the paper by Siteng Huang et. al.","category":"page"},{"location":"#Quickstart","page":"Home","title":"Quickstart","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Activate the package and load some sample-data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FluxArchitectures\npoollength = 10; horizon = 15; datalength = 1000;\ninput, target = get_data(:exchange_rate, poollength, datalength, horizon) ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Define a model and a loss function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"model = LSTnet(size(input, 1), 2, 3, poollength, 120)\nloss(x, y) = Flux.mse(model(x), y')","category":"page"},{"location":"","page":"Home","title":"Home","text":"Train the model:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Flux.train!(loss, Flux.params(model),Iterators.repeated((input, target), 20), Adam(0.01))","category":"page"},{"location":"datasets/datasets/#Datasets","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"datasets/datasets/","page":"Datasets","title":"Datasets","text":"Currently, the following example datasets from github.com/laiguokun/multivariate-time-series-data are included:","category":"page"},{"location":"datasets/datasets/","page":"Datasets","title":"Datasets","text":":solar: The raw data is coming from www.nrel.gov/grid/solar-power-data.html: It contains the solar power production records in the year of 2006, which is sampled every 10 minutes from 137 PV plants in Alabama State.\n:traffic: The raw data is coming from pems.dot.ca.gov. The data in this repo is a collection of 48 months (2015-2016) hourly data from the California Department of Transportation. The data describes the road occupancy rates (between 0 and 1) measured by different sensors on San Francisco Bay area freeways.\n:electricity: The raw dataset is from archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014. It is the electricity consumption in kWh was recorded every 15 minutes from 2011 to 2014 for 321 clients. The data has been cleaned and converted to hourly consumption.\n:exchange_rate: The collection of daily exchange rates of eight foreign countries including Australia, Great Britain, Canada, Switzerland, China, Japan, New Zealand and Singapore ranging from 1990 to 2016.","category":"page"}]
}
