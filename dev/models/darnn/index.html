<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>DARNN · FluxArchitectures.jl</title><meta name="title" content="DARNN · FluxArchitectures.jl"/><meta property="og:title" content="DARNN · FluxArchitectures.jl"/><meta property="twitter:title" content="DARNN · FluxArchitectures.jl"/><meta name="description" content="Documentation for FluxArchitectures.jl."/><meta property="og:description" content="Documentation for FluxArchitectures.jl."/><meta property="twitter:description" content="Documentation for FluxArchitectures.jl."/><meta property="og:url" content="https://sdobber.github.io/FluxArchitectures.jl/models/darnn/"/><meta property="twitter:url" content="https://sdobber.github.io/FluxArchitectures.jl/models/darnn/"/><link rel="canonical" href="https://sdobber.github.io/FluxArchitectures.jl/models/darnn/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">FluxArchitectures.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../examples/examples/">Examples</a></li><li><a class="tocitem" href="../../datasets/datasets/">Datasets</a></li><li><a class="tocitem" href="../../functions/">Exported Functions</a></li><li><span class="tocitem">Models</span><ul><li class="is-active"><a class="tocitem" href>DARNN</a><ul class="internal"><li><a class="tocitem" href="#Network-Structure"><span>Network Structure</span></a></li><li><a class="tocitem" href="#Known-Issues"><span>Known Issues</span></a></li></ul></li><li><a class="tocitem" href="../dsanet/">DSANet</a></li><li><a class="tocitem" href="../lstnet/">LSTnet</a></li><li><a class="tocitem" href="../tpalstm/">TPALSTM</a></li></ul></li><li><a class="tocitem" href="../../benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Models</a></li><li class="is-active"><a href>DARNN</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>DARNN</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sdobber/FluxArchitectures.jl/blob/master/docs/src/models/darnn.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="DARNN"><a class="docs-heading-anchor" href="#DARNN">DARNN</a><a id="DARNN-1"></a><a class="docs-heading-anchor-permalink" href="#DARNN" title="Permalink"></a></h1><p>The &quot;Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction&quot; is based on the paper by <a href="https://arxiv.org/abs/1704.02971">Qin et. al.</a>. See also the corresponding <a href="https://sdobber.github.io/FA_DARNN/">blog post</a>.</p><p>The code is based on a <a href="https://github.com/Seanny123/da-rnn/blob/master/modules.py">PyTorch implementation</a> of the same model with slight adjustments.</p><h2 id="Network-Structure"><a class="docs-heading-anchor" href="#Network-Structure">Network Structure</a><a id="Network-Structure-1"></a><a class="docs-heading-anchor-permalink" href="#Network-Structure" title="Permalink"></a></h2><p>The neural network has a rather complex structure. Starting with an encoder-decoder structure, it consists of two units, one called the <em>input attention mechanism</em>, and a <em>temporal attention mechanism</em>.</p><ul><li><p>The input attention mechanism feeds the input data to a LSTM network. In subsequent calculations, only its hidden state is used, where additional network layers try to estimate the importance of different hidden variables.</p></li><li><p>The temporal attention mechanism takes the hidden state of the encoder network and combines it with the hidden state of another LSTM decoder. Additional network layers try again to estimate the importance of the hidden variables of the encoder and decoder combined.</p></li><li><p>Linear layers combine the output of different layers to the final time series prediction.</p></li></ul><p>The <strong>encoder</strong> part made up of the following:</p><ul><li>A <code>LSTM</code> encoder layer. Only the hidden state is used for the output of the network.</li><li>A feature attention layer consisting of two <code>Dense</code> layers with <code>tanh</code> activation function for the first layer.</li></ul><p><img src="https://pic2.zhimg.com/80/v2-4e0c7c8fb419bb91a218d9a295b85fa9_1440w.jpg" alt="Model Structure Encoder"/></p><blockquote><p>Encoder Structure. Image from Qin et. al., &quot;Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction&quot;, <a href="https://arxiv.org/abs/1704.02971">ArXiv</a>, 2017.</p></blockquote><p>The <strong>decoder</strong> part consist of</p><ul><li>A <code>LSTM</code> decoder layer. It&#39;s hidden state is used for determining a scaling of the original time series.</li><li>A temporal attention layer operating on the hidden state of the encoder layer, consisting of two <code>Dense</code> layers similar to the encoder.</li><li>A <code>Dense</code> layer operating on the encoder output and the temporal attention layer. Its ouput gets fed into the decoder.</li><li>A <code>Dense</code> layer to obtain the final output based on the hidden state of the decoder.</li></ul><p><img src="https://pic2.zhimg.com/80/v2-1ac7ca28be64000bf7b02e8e9d0c752d_1440w.jpg" alt="Decoder Structure Encoder"/></p><blockquote><p>Decoder Structure. Image from Qin et. al., &quot;Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction&quot;, <a href="https://arxiv.org/abs/1704.02971">ArXiv</a>, 2017.</p></blockquote><h2 id="Known-Issues"><a class="docs-heading-anchor" href="#Known-Issues">Known Issues</a><a id="Known-Issues-1"></a><a class="docs-heading-anchor-permalink" href="#Known-Issues" title="Permalink"></a></h2><p>Training of the <code>DARNN</code> layer is very slow, probably due to the use of <code>Zygote.Buffer</code> in the encoder.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../functions/">« Exported Functions</a><a class="docs-footer-nextpage" href="../dsanet/">DSANet »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.2 on <span class="colophon-date" title="Wednesday 22 November 2023 16:47">Wednesday 22 November 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
